
# K-Nearest Neighbors - Recap

## Introduction

In this lesson you'll briefly review some of the key concepts covered in this section including KNN's computational complexity and how to properly tune a model using scikit-learn. 

## K-Nearest Neighbors

As you saw, KNN is an intuitive algorithm: to generate a prediction for a given data point, it finds the k-nearest data points and then predicts the majority class of these k points.

### Computational complexity

Also of note is the computational complexity of the KNN algorithm. As the number of data points and features increase, the required calculations increases exponentially! As such, KNN is extremely resource intensive for large datasets.

## Distance metrics

You learned about Minkowski distance and two cases of Minkowski distance: Euclidean and Manhattan distance. Other distance metrics such as Hamming distance can even be used to compare strings! (Hamming distance can be used to offer typo correction-suggestions for instance by comparing similar words generated by changing only one or two letters from the mistyped word). 

## Model tuning in scikit-learn 

Remember that model tuning encapsulates the entire gamut of the data science process from problem formulation and preprocessing through hyperparameter tuning. Furthermore, you also need to choose a validation method to determine the model's ability to generalize to new cases such as train-test split or cross-validation. Good models require careful thought, ample preprocessing, and exploration followed by hyperparameter tuning.

## Summary

Well done! You have added another algorithm in your toolset. Even though KNN doesn't scale well to larger datasets, it has many useful applications from recommendations to classification. 
